{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2Av2pcoJ+EWTAVu43P57J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RamyaRamasubramaniyan/PysparkProjects/blob/main/Spark_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "yeILBk-_plyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "7reDWumerZZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = (SparkSession.builder\n",
        "        .master(\"local\")\n",
        "        .appName(\"Colab\")\n",
        "        .config('spark.ui.port', '4050')\n",
        "        .getOrCreate())"
      ],
      "metadata": {
        "id": "hW-YB57qrbrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "id": "NJbNT3g0rbyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, date\n",
        "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, DateType, DoubleType, BooleanType, DecimalType\n",
        "from pyspark.sql.functions import lit, col, when, sum as sum_, max as max_, last, datediff, to_date\n",
        "from pyspark.sql import Window"
      ],
      "metadata": {
        "id": "vieDESRry3z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data = [(1, \"Ramya\",  2000.00, \"Tamil\", True, 2, date(2024, 1, 31)),\n",
        "               (2, \"Willem\", 1000.53, \"German\", False, 0, date(2023, 12, 31)),\n",
        "               (3, \"Saskia\", 100.00, \"Dutch\", True, 1, date(2024, 4, 21)),\n",
        "               (4, \"Bianca\", 10.64, \"English\", True, 5, date(2024, 4, 21)),\n",
        "               (5, \"Jeroen\", 270.05, \"French\", True, 4, date(2024, 5, 30)),\n",
        "               (5, \"Jeroen\", 270.05, \"French\", True, 4, date(2024, 5, 30)),\n",
        "               (6, \"Saskia\", 400.00, \"Japanese\", True, 2, date(2024, 3, 13)),\n",
        "               (7, \"Tristan\", 5.00, \"English\", False, 0, None),\n",
        "               (8, \"Danil\", 20.05, \"Roman\", True, 4, date(2024, 1, 30)),\n",
        "               (9, \"Danil\", 80.10, \"Thai\", True, 2, date(2024, 2, 3)),\n",
        "               ]\n",
        "print(sample_data)"
      ],
      "metadata": {
        "id": "sBlEjXr9rqAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_schema = StructType([StructField(\"SNo\", IntegerType(), False),\n",
        "                           StructField(\"Borrower_Name\", StringType(), False),\n",
        "                           StructField(\"Library_Credits\", DoubleType(), True),\n",
        "                           StructField(\"Language\", StringType(), True),\n",
        "                           StructField(\"Has_Borrowed\", BooleanType(), True),\n",
        "                           StructField(\"Borrowed_Books_Count\", IntegerType(), True),\n",
        "                           StructField(\"Book_Due_Date\", DateType(), True),\n",
        "                           ])\n",
        "print(sample_schema)"
      ],
      "metadata": {
        "id": "ILdnG8xCztlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(data=sample_data, schema=sample_schema)"
      ],
      "metadata": {
        "id": "R5mXJzdfpfdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(10, False)"
      ],
      "metadata": {
        "id": "81XaXLxWzcy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fields = df.columns"
      ],
      "metadata": {
        "id": "FhUeuc0RH-0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # csv writer using python\n",
        "# import csv\n",
        "# sample_header = fields\n",
        "# with open(\"/content/drive/MyDrive/Pyspark/Library_Sample_Data.csv\", newline='', mode='w+') as myfile:\n",
        "#   wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
        "#   wr.writerow(sample_header)\n",
        "#   wr.writerows(sample_data)"
      ],
      "metadata": {
        "id": "NtWJp78TGwcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "S# csv writer using dataframe writes compressed csv\n",
        "# df.write.csv(\"/content/Library_Sample_Data.csv\")"
      ],
      "metadata": {
        "id": "wyaXgKGdIMaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "id": "2Th8nKrrzhwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.distinct().count()"
      ],
      "metadata": {
        "id": "ODi7Cwnd3foX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_df = df.dropDuplicates(subset=[\"Borrower_Name\"])\n",
        "unique_df.orderBy(\"SNo\").show()\n",
        "unique_df.count()"
      ],
      "metadata": {
        "id": "PZC-V_rz3h3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_data = df.groupBy(\"Borrower_Name\", \"Language\").agg(sum_(\"Library_Credits\").alias(\"Total_Library_Credits\"),\n",
        "                                                           sum_(\"Borrowed_Books_Count\").alias(\"Total_Borrowed_Books_Count\")\n",
        "                                                           )\n",
        "grouped_data.orderBy(\"Borrower_Name\").show()"
      ],
      "metadata": {
        "id": "h4_IILp13zQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "today = datetime.now().date()\n",
        "tranform_data = (df\n",
        "                 .withColumn(\"Due_Fine_Flag\", when((col(\"Book_Due_Date\") < today)\n",
        "                                                      & (col(\"Has_Borrowed\") == True), True).otherwise(False))\n",
        "                 .withColumn(\"Due_Fine_Amount\", (when((col(\"Book_Due_Date\") < today)\n",
        "                                                      & (col(\"Has_Borrowed\") == True),\n",
        "                                                   datediff(lit(today), col(\"Book_Due_Date\")) * 5 * col(\"Borrowed_Books_Count\") )\n",
        "                                                .otherwise(lit(0))\n",
        "                                                ).cast(DecimalType(10, 2))\n",
        "                 ))\n",
        "\n",
        "tranform_data.orderBy(\"SNo\").show()"
      ],
      "metadata": {
        "id": "XmMYzo_v4d3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using groupby\n",
        "grouped_data = (tranform_data\n",
        "                .groupBy(\"Borrower_Name\", \"Language\")\n",
        "                 .agg(sum_(\"Library_Credits\").alias(\"Total_Library_Credits\"),\n",
        "                      sum_(\"Borrowed_Books_Count\").alias(\"Total_Borrowed_Books_Count\"),\n",
        "                      last(\"Due_Fine_Flag\").alias(\"Due_Fine_Flag\"),\n",
        "                      sum_(\"Due_Fine_Amount\").alias(\"Total_Due_Fine_Amount\")\n",
        "                      )\n",
        ").filter(col(\"Due_Fine_Flag\"))\n",
        "grouped_data.orderBy(\"Borrower_Name\").show()"
      ],
      "metadata": {
        "id": "QgDl_vNm-hSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using window function\n",
        "window = Window.partitionBy(\"Borrower_Name\", \"Language\").orderBy(\"Book_Due_Date\")\n",
        "df_window = (tranform_data\n",
        "             .withColumn(\"Total_Library_Credits\", sum_(\"Library_Credits\").over(window))\n",
        "             .withColumn(\"Total_Borrowed_Books_Count\", sum_(\"Borrowed_Books_Count\").over(window))\n",
        "             .withColumn(\"Total_Due_Fine_Amount\", sum_(\"Due_Fine_Amount\").over(window))\n",
        "             ).filter(col(\"Due_Fine_Flag\"))\n",
        "df_window.orderBy(\"Borrower_Name\").show()"
      ],
      "metadata": {
        "id": "g0IB9cRj6UwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a class\n",
        "import abc\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, DateType, DoubleType, BooleanType, DecimalType\n",
        "from pyspark.sql.functions import lit, col, when, sum as sum_, max as max_, last, datediff, to_date\n",
        "from pyspark.sql import Window\n",
        "\n",
        "class SparkSessionCreation():\n",
        "\n",
        "    def _spark_session():\n",
        "      return (SparkSession.builder\n",
        "              .master(\"local\")\n",
        "              .appName(\"Colab\")\n",
        "              .config('spark.ui.port', '4050')\n",
        "              .getOrCreate())\n",
        "\n",
        "\n",
        "class LibraryManagementfunctions(SparkSessionCreation):\n",
        "\n",
        "    def __init__(self):\n",
        "      self.spark = super()._spark_session\n",
        "\n",
        "    def run_pipeline(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _due_fine_calculator(self):\n",
        "        pass\n",
        "\n",
        "    def _due_fine_checker(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class LibraryManagement(LibraryManagementfunctions):\n",
        "\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      today = datetime.now().date()\n",
        "      print(self.spark)\n",
        "\n",
        "    def run_pipeline(self):\n",
        "      return (self._read_source()\n",
        "              .tranform(self._due_fine_calculator)\n",
        "              .transform(self._due_fine_calculator)\n",
        "              )\n",
        "\n",
        "    def _read_source(self):\n",
        "      return (self\n",
        "              .spark()\n",
        "              .read\n",
        "              .option(\"delimiter\", \",\")\n",
        "              .option(\"header\", \"true\")\n",
        "              .csv(\"/content/drive/MyDrive/Pyspark/Library_Sample_Data.csv\"))\n",
        "\n",
        "    def _due_fine_calculator(self):\n",
        "      return(df\n",
        "             .withColumn(\"Due_Fine_Flag\", when((col(\"Book_Due_Date\") < self.today)\n",
        "                                                & (col(\"Has_Borrowed\") == True), True).otherwise(False))\n",
        "             .withColumn(\"Due_Fine_Amount\", (when((col(\"Book_Due_Date\") < self.today)\n",
        "                                                 & (col(\"Has_Borrowed\") == True),\n",
        "                                                  datediff(lit(today), col(\"Book_Due_Date\")) * 5 * col(\"Borrowed_Books_Count\") )\n",
        "                                                             .otherwise(lit(0))\n",
        "                                            ).cast(DecimalType(10, 2))\n",
        "              ))\n",
        "\n",
        "    @staticmethod\n",
        "    def _due_fine_checker(df):\n",
        "      window = Window.partitionBy(\"Borrower_Name\", \"Language\").orderBy(\"Book_Due_Date\")\n",
        "      return (df\n",
        "             .withColumn(\"Total_Library_Credits\", sum_(\"Library_Credits\").over(window))\n",
        "             .withColumn(\"Total_Borrowed_Books_Count\", sum_(\"Borrowed_Books_Count\").over(window))\n",
        "             .withColumn(\"Total_Due_Fine_Amount\", sum_(\"Due_Fine_Amount\").over(window))\n",
        "             ).filter(col(\"Due_Fine_Flag\"))\n"
      ],
      "metadata": {
        "id": "Xk76Zo0Y_KGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj = LibraryManagement()\n",
        "obj.run_pipeline()"
      ],
      "metadata": {
        "id": "D1SUH5YRCzqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dCKZ9lY-C7-D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}